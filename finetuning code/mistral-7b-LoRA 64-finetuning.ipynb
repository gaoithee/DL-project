{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNFfJyh6QC2o"
   },
   "outputs": [],
   "source": [
    "# SOLO PER COLAB\n",
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports necessari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeQVeE47Z5tK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-11 17:18:58.592360: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-11 17:18:58.637497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-11 17:18:58.637528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-11 17:18:58.638797: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-11 17:18:58.647778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-11 17:19:12.770693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EweYNpZadyU"
   },
   "source": [
    "## fissa i parametri e i nomi dei modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dusws6WiZ9El"
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"mistral-7b-LoRA-64-harrypotter\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results_mistral-7b\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 5\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 8\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 50\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 50\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "fe88bdebc8084b36906dc9702729eb66",
      "1d5d678c78d34f4ea6ddd31f752df4c1",
      "24c8a79d859a4828a33486c17f5e12fb",
      "446beac29cb34232b5d3de1c8af795c8",
      "f336a484b5b84166b18fc203bdc54b3c",
      "23881cddb9ee4a3e8c73607f69141efb",
      "53b008a92394462ebc2abfdfb78d935b",
      "297c3fbe80fa4ddf80ae339b38c1e0a5",
      "776c1809f6264949a3f22aeb956fcaaf",
      "d90242525ede4a609090c03bfeed942a",
      "f0a5b93c4a7f409ea2c59c0260fd3b0a"
     ]
    },
    "id": "iF1cqGRbbMvK",
    "outputId": "0cd302d9-e227-4740-8bee-03670bb9c842"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|                              | 0/2 [00:00<?, ?it/s]\n",
      "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%| | 10.5M/3.50G [00:00<00:49, 71.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%| | 31.5M/3.50G [00:00<01:21, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%| | 41.9M/3.50G [00:00<01:08, 50.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%| | 62.9M/3.50G [00:01<01:23, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%| | 73.4M/3.50G [00:01<01:10, 48.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%| | 83.9M/3.50G [00:01<01:11, 47.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|  | 105M/3.50G [00:02<01:16, 44.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|  | 115M/3.50G [00:02<01:08, 49.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|  | 136M/3.50G [00:02<01:16, 43.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|  | 157M/3.50G [00:03<01:06, 50.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|  | 178M/3.50G [00:03<00:53, 62.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|  | 189M/3.50G [00:04<01:15, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|  | 210M/3.50G [00:04<01:19, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▏ | 231M/3.50G [00:04<01:03, 51.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▏ | 252M/3.50G [00:05<01:29, 36.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▏ | 262M/3.50G [00:05<01:22, 39.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▏ | 283M/3.50G [00:06<01:47, 29.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▏ | 304M/3.50G [00:07<01:39, 32.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▏ | 325M/3.50G [00:07<01:16, 41.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▏ | 336M/3.50G [00:08<01:28, 35.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▏ | 357M/3.50G [00:08<01:27, 35.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▏ | 377M/3.50G [00:08<01:09, 45.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▏ | 398M/3.50G [00:09<00:54, 57.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▏ | 409M/3.50G [00:09<01:34, 32.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▏ | 430M/3.50G [00:11<01:59, 25.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▎ | 451M/3.50G [00:11<01:44, 29.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▎ | 461M/3.50G [00:11<01:32, 32.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▎ | 482M/3.50G [00:12<01:19, 37.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▎ | 503M/3.50G [00:12<01:21, 36.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▎ | 514M/3.50G [00:12<01:12, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▎ | 535M/3.50G [00:13<00:55, 53.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▎ | 556M/3.50G [00:13<01:05, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▎ | 577M/3.50G [00:14<01:09, 42.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▎ | 598M/3.50G [00:14<01:11, 40.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▎ | 619M/3.50G [00:15<00:56, 50.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▎ | 629M/3.50G [00:15<00:51, 55.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▎ | 650M/3.50G [00:16<01:20, 35.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▍ | 671M/3.50G [00:17<01:37, 29.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▍ | 682M/3.50G [00:17<01:23, 33.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|▍ | 703M/3.50G [00:18<01:41, 27.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|▍ | 724M/3.50G [00:18<01:32, 30.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|▍ | 734M/3.50G [00:18<01:19, 34.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|▍ | 744M/3.50G [00:19<01:23, 33.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|▍ | 755M/3.50G [00:19<01:12, 37.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|▍ | 765M/3.50G [00:19<01:03, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|▍ | 776M/3.50G [00:20<01:16, 35.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|▍ | 786M/3.50G [00:20<01:02, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|▍ | 797M/3.50G [00:20<01:03, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|▍ | 807M/3.50G [00:20<00:55, 48.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|▍ | 818M/3.50G [00:20<01:07, 39.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|▍ | 828M/3.50G [00:21<00:58, 45.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|▍ | 839M/3.50G [00:21<01:04, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|▍ | 860M/3.50G [00:21<01:07, 39.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|▍ | 870M/3.50G [00:22<00:58, 45.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|▌ | 891M/3.50G [00:22<01:04, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|▌ | 912M/3.50G [00:22<00:49, 52.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|▌ | 923M/3.50G [00:23<00:47, 54.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|▌ | 933M/3.50G [00:23<01:22, 31.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|▌ | 954M/3.50G [00:24<00:58, 43.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|▌ | 965M/3.50G [00:24<01:34, 26.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|▌ | 986M/3.50G [00:25<01:44, 24.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|▎| 1.01G/3.50G [00:26<01:48, 22.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|▎| 1.02G/3.50G [00:27<01:31, 27.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|▎| 1.04G/3.50G [00:27<01:21, 30.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|▎| 1.06G/3.50G [00:28<01:15, 32.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|▎| 1.08G/3.50G [00:28<01:11, 34.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|▎| 1.09G/3.50G [00:28<01:02, 38.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|▎| 1.11G/3.50G [00:29<00:48, 48.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|▎| 1.13G/3.50G [00:30<01:11, 33.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|▎| 1.15G/3.50G [00:30<01:08, 34.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|▎| 1.16G/3.50G [00:30<01:00, 38.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|▎| 1.18G/3.50G [00:31<01:00, 38.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|▎| 1.21G/3.50G [00:31<00:51, 44.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|▎| 1.23G/3.50G [00:32<00:49, 45.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|▎| 1.25G/3.50G [00:32<00:39, 57.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|▎| 1.26G/3.50G [00:33<01:08, 32.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|▎| 1.28G/3.50G [00:33<01:04, 34.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|▎| 1.30G/3.50G [00:33<00:49, 44.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|▎| 1.31G/3.50G [00:34<00:58, 37.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|▍| 1.33G/3.50G [00:35<00:59, 36.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|▍| 1.35G/3.50G [00:35<00:45, 46.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|▍| 1.37G/3.50G [00:35<00:36, 58.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|▍| 1.38G/3.50G [00:36<01:04, 32.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|▍| 1.41G/3.50G [00:36<01:01, 34.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|▍| 1.42G/3.50G [00:36<00:52, 39.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|▍| 1.43G/3.50G [00:37<01:01, 34.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|▍| 1.44G/3.50G [00:37<00:51, 39.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|▍| 1.45G/3.50G [00:37<00:43, 47.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|▍| 1.46G/3.50G [00:38<00:56, 36.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|▍| 1.48G/3.50G [00:38<00:40, 49.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|▍| 1.50G/3.50G [00:38<00:45, 43.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|▍| 1.51G/3.50G [00:39<00:39, 50.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|▍| 1.53G/3.50G [00:39<00:46, 42.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|▍| 1.55G/3.50G [00:40<00:46, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|▍| 1.57G/3.50G [00:40<00:37, 51.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|▍| 1.59G/3.50G [00:40<00:30, 62.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|▍| 1.60G/3.50G [00:41<00:56, 33.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|▍| 1.63G/3.50G [00:42<01:02, 30.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|▍| 1.64G/3.50G [00:42<00:54, 34.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|▍| 1.66G/3.50G [00:43<01:06, 27.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|▍| 1.68G/3.50G [00:43<00:49, 37.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|▍| 1.69G/3.50G [00:44<00:56, 32.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|▍| 1.71G/3.50G [00:44<00:52, 33.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|▍| 1.72G/3.50G [00:44<00:45, 39.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|▍| 1.73G/3.50G [00:45<00:50, 35.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|▌| 1.75G/3.50G [00:45<00:36, 48.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|▌| 1.76G/3.50G [00:45<00:36, 47.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|▌| 1.78G/3.50G [00:46<00:43, 39.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|▌| 1.79G/3.50G [00:46<00:38, 44.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|▌| 1.81G/3.50G [00:46<00:28, 58.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|▌| 1.84G/3.50G [00:47<00:46, 35.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|▌| 1.86G/3.50G [00:48<00:58, 28.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|▌| 1.87G/3.50G [00:48<00:50, 32.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|▌| 1.88G/3.50G [00:49<00:54, 29.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|▌| 1.90G/3.50G [00:49<00:38, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|▌| 1.91G/3.50G [00:49<00:45, 34.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|▌| 1.93G/3.50G [00:50<00:36, 43.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|▌| 1.95G/3.50G [00:50<00:37, 40.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|▌| 1.96G/3.50G [00:50<00:33, 46.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|▌| 1.97G/3.50G [00:51<00:29, 52.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|▌| 1.98G/3.50G [00:51<00:37, 40.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|▌| 2.00G/3.50G [00:52<00:39, 38.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|▌| 2.01G/3.50G [00:52<00:34, 43.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|▌| 2.03G/3.50G [00:52<00:25, 57.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|▌| 2.04G/3.50G [00:53<00:46, 31.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|▌| 2.07G/3.50G [00:53<00:38, 36.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|▌| 2.08G/3.50G [00:54<00:43, 32.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|▌| 2.10G/3.50G [00:54<00:32, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|▌| 2.12G/3.50G [00:55<00:42, 32.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|▌| 2.14G/3.50G [00:55<00:32, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|▌| 2.15G/3.50G [00:56<00:48, 28.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|▌| 2.16G/3.50G [00:56<00:40, 32.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|▌| 2.17G/3.50G [00:57<00:46, 28.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|▌| 2.18G/3.50G [00:57<00:37, 35.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|▋| 2.20G/3.50G [00:57<00:37, 34.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|▋| 2.21G/3.50G [00:57<00:32, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|▋| 2.23G/3.50G [00:58<00:23, 54.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|▋| 2.24G/3.50G [00:58<00:42, 29.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|▋| 2.26G/3.50G [01:00<00:48, 25.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|▋| 2.28G/3.50G [01:00<00:40, 30.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|▋| 2.30G/3.50G [01:00<00:37, 32.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|▋| 2.31G/3.50G [01:00<00:32, 36.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|▋| 2.32G/3.50G [01:01<00:36, 32.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|▋| 2.33G/3.50G [01:01<00:31, 37.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|▋| 2.35G/3.50G [01:01<00:22, 50.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|▋| 2.36G/3.50G [01:02<00:38, 30.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|▋| 2.37G/3.50G [01:02<00:32, 34.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|▋| 2.39G/3.50G [01:03<00:31, 35.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|▋| 2.40G/3.50G [01:03<00:26, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|▋| 2.41G/3.50G [01:03<00:28, 37.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|▋| 2.42G/3.50G [01:03<00:25, 41.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|▋| 2.44G/3.50G [01:04<00:29, 36.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|▋| 2.46G/3.50G [01:04<00:21, 47.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|▋| 2.49G/3.50G [01:05<00:30, 32.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|▋| 2.50G/3.50G [01:05<00:26, 38.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|▋| 2.51G/3.50G [01:06<00:30, 32.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|▋| 2.53G/3.50G [01:06<00:21, 44.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|▋| 2.54G/3.50G [01:06<00:26, 36.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|▋| 2.56G/3.50G [01:07<00:25, 37.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|▋| 2.58G/3.50G [01:07<00:19, 48.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|▋| 2.60G/3.50G [01:07<00:14, 60.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|▋| 2.61G/3.50G [01:08<00:27, 32.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|▊| 2.63G/3.50G [01:09<00:25, 34.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|▊| 2.65G/3.50G [01:09<00:24, 35.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|▊| 2.67G/3.50G [01:10<00:18, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|▊| 2.68G/3.50G [01:10<00:21, 37.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|▊| 2.71G/3.50G [01:10<00:16, 48.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|▊| 2.72G/3.50G [01:10<00:14, 54.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|▊| 2.73G/3.50G [01:11<00:19, 40.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|▊| 2.75G/3.50G [01:11<00:13, 53.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|▊| 2.76G/3.50G [01:12<00:18, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|▊| 2.78G/3.50G [01:12<00:18, 38.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|▊| 2.80G/3.50G [01:12<00:14, 48.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|▊| 2.82G/3.50G [01:13<00:20, 33.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|▊| 2.83G/3.50G [01:14<00:17, 38.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|▊| 2.85G/3.50G [01:14<00:18, 35.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|▊| 2.87G/3.50G [01:15<00:15, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|▊| 2.89G/3.50G [01:15<00:15, 40.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  83%|▊| 2.92G/3.50G [01:15<00:11, 50.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|▊| 2.93G/3.50G [01:16<00:14, 40.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|▊| 2.95G/3.50G [01:16<00:11, 47.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|▊| 2.97G/3.50G [01:17<00:12, 43.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|▊| 2.99G/3.50G [01:17<00:09, 53.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|▊| 3.00G/3.50G [01:17<00:11, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|▊| 3.02G/3.50G [01:18<00:09, 48.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|▊| 3.04G/3.50G [01:18<00:09, 46.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|▊| 3.05G/3.50G [01:18<00:09, 49.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|▊| 3.06G/3.50G [01:18<00:07, 55.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|▉| 3.07G/3.50G [01:19<00:10, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  88%|▉| 3.09G/3.50G [01:19<00:10, 39.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|▉| 3.11G/3.50G [01:20<00:07, 51.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|▉| 3.12G/3.50G [01:20<00:06, 57.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|▉| 3.15G/3.50G [01:21<00:09, 35.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  90%|▉| 3.17G/3.50G [01:22<00:11, 28.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|▉| 3.19G/3.50G [01:22<00:08, 38.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|▉| 3.20G/3.50G [01:22<00:09, 33.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|▉| 3.22G/3.50G [01:23<00:08, 34.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|▉| 3.24G/3.50G [01:24<00:07, 34.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|▉| 3.26G/3.50G [01:24<00:05, 44.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|▉| 3.27G/3.50G [01:24<00:04, 50.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|▉| 3.29G/3.50G [01:25<00:06, 31.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|▉| 3.31G/3.50G [01:25<00:05, 33.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|▉| 3.33G/3.50G [01:26<00:03, 42.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|▉| 3.34G/3.50G [01:26<00:04, 36.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|▉| 3.37G/3.50G [01:26<00:02, 47.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|▉| 3.38G/3.50G [01:26<00:02, 52.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|▉| 3.39G/3.50G [01:27<00:03, 29.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|▉| 3.41G/3.50G [01:28<00:02, 41.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|▉| 3.42G/3.50G [01:28<00:02, 34.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|▉| 3.44G/3.50G [01:29<00:01, 35.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|▉| 3.46G/3.50G [01:29<00:01, 38.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  99%|▉| 3.47G/3.50G [01:29<00:00, 40.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|█| 3.50G/3.50G [01:30<00:00, 38.8MB/s]\u001b[A\n",
      "Downloading shards: 100%|██████████████████████| 2/2 [01:30<00:00, 45.23s/it]\n",
      "Loading checkpoint shards: 100%|███████████████| 2/2 [01:04<00:00, 32.21s/it]\n",
      "generation_config.json: 100%|███████████████| 179/179 [00:00<00:00, 1.31MB/s]\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "tokenizer_config.json: 100%|████████████████| 746/746 [00:00<00:00, 5.86MB/s]\n",
      "tokenizer.model: 100%|████████████████████| 500k/500k [00:00<00:00, 43.6MB/s]\n",
      "tokenizer.json: 100%|███████████████████| 1.84M/1.84M [00:00<00:00, 4.58MB/s]\n",
      "added_tokens.json: 100%|███████████████████| 21.0/21.0 [00:00<00:00, 190kB/s]\n",
      "special_tokens_map.json: 100%|██████████████| 435/435 [00:00<00:00, 4.04MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    # forse ti si romperà qui! \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnWjzuQCajMG"
   },
   "source": [
    "## importa il dataset e lo preprocessa opportunamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYEAbJ3ZaZ4v",
    "outputId": "d4c11e49-5efc-40b3-e510-cf771d677271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'text'],\n",
       "    num_rows: 1023\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = load_dataset(\"saracandu/harry-potter-trivia-ai\", split=\"train\")\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## allena il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I2ZlLA-lbXoe",
    "outputId": "e66f6af8-f691-4d32-a20f-85a61c1a602f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████████████████| 1023/1023 [00:00<00:00, 3338.10 examples/s]\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 03:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.402300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/u/dssc/scandu00/.conda/envs/DLenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=df,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unisce i parametri re-trained a quelli base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UL00iN2igM6K",
    "outputId": "da29609f-8d0a-4a7b-ac79-e071b297ef97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is Hogwarts? [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST] A school for wizards and witches. everybody! [/INST\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"What is Hogwarts?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "# forse non ha più questo campo qui\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qua ti conviene riavviare il notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## carica il modello su HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "4b832dede58c4a0282db1036d8eefe8f",
      "99476038546046c0992446c38cbe37a9",
      "d4e39270d388470686a408d661842333",
      "288bfdf3f37a40c098ec35f963517a73",
      "3293c750fe5c4485bb6f6d3c426c14bb",
      "3f443ac9bca845a1ad3c900f6c9ab116",
      "d1226ac3acb64330af776abb65f70b53",
      "eff5233485974292a20748b9fefe21c0",
      "6853bfc4a9da48b895f8aabc0caa435d",
      "d714b2992d9f408bbe83271a8763a1c5",
      "c39b49bc34ed4c0e8627a372654ecbc7"
     ]
    },
    "id": "hnrMMJgyn7na",
    "outputId": "aa4d1a28-1df4-4d65-b03d-bf58654990e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b832dede58c4a0282db1036d8eefe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mshiwhJNqKs7"
   },
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617,
     "referenced_widgets": [
      "aca5f66799734cdd8de81db924fe774c",
      "80a8484f292f439580c8691d2271ed7c",
      "fb13461a6db94578b6bcb26e11ec8a90",
      "39b8896477594c6fb5221616a48e5d0c",
      "c8e12355dff84121bd117ab1d912f878",
      "a0b7c4eff1954b039e6b0f14a99dd6ff",
      "78021b9489c140f3a7fdbb68536791a4",
      "f4aba009134740f4b4966ff1fb2d56e1",
      "990ed5ebb5e246aa9985953975412cc7",
      "b579594369b8466fb4bc7b50a01e7213",
      "ffb981ebd74c4332aa3f23a32b528457",
      "262dcb631baf4c6b92f1dac3ceabbd3d",
      "48cb14c49d134499837c87e46d2c8400",
      "4c43e1357a3b48c29babc0e7689207e6",
      "83342163006c424d91aef57c1ba9918a",
      "68d5397064e24d9fb905b5486b75fba0",
      "0795becfdc104398b5ba90fc17f329d4",
      "4e3b23eb23b84628a4358a8694e1dd50",
      "12107925be26432ebb428bf5b93c1b27",
      "621faee2d8494f11a25d1eec913a63cf",
      "aa3623aeb30c44fa8aff1cb9d322cac3",
      "dcd02f7bf571493dbd8c88127edfc0a3",
      "085f75a6397b4abeacbeb494d03ce9f2",
      "36a92856535f4a3e9d226a13c50e7ccb",
      "830d58d1bec64358a477137af61075e0",
      "0fdc71780a534925a887d0aa02dac468",
      "1ea19b02426a417a808f9bc0cd650632",
      "23e57fd771874c898c415eb71e10ec41",
      "479a70e5447642c38829bbec0a24fe09",
      "fa51f1bd6e384fc789b171fd7c60dbec",
      "e3c1f72d5b4e4e1bb18ea91f482c401a",
      "c3c06a176f374c96a83d3408354fc9b8",
      "2c0e5bb4670b4162841358b3fc37236e",
      "24cf4da9e8d84d229288a4ac99584dec",
      "c956fe49c84e4ec49bb7b2a4504d9fe0",
      "94af7ca7e1174bada19c0e106e62c85d",
      "c008a4f6ba0e4e3aa0005ae584fd0a0b",
      "300591bc31c848ec97c7869f36f8881f",
      "663f91870a464b509aae84a608f2ef73",
      "dc89fdb27a99488581edec0cf13d6d65",
      "5dba9f1d19b64882a7b658e919fb8d1f",
      "9a5edb8034574e59a374d893ab0475dc",
      "3b75776849724e4cb11bb90c21ad577f",
      "e60f3f4fdfb44cd68080b9c9d6f1ec86"
     ]
    },
    "id": "2hSP60kdqUu7",
    "outputId": "430242bf-7b47-4919-8a2d-8b708834ace0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token: \n",
      "Add token as git credential? (Y/n) Y\n",
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca5f66799734cdd8de81db924fe774c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262dcb631baf4c6b92f1dac3ceabbd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085f75a6397b4abeacbeb494d03ce9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cf4da9e8d84d229288a4ac99584dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/saracandu/llama-2-7b-harrypotter/commit/79b520bbb3305d2db277f21b8bf9f1724dc99683', commit_message='Upload tokenizer', commit_description='', oid='79b520bbb3305d2db277f21b8bf9f1724dc99683', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
